{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import hmac\n",
    "import time\n",
    "import base64\n",
    "from urllib.parse import quote_plus\n",
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurar el logger\n",
    "logging.basicConfig(filename='whalewisdom_api.log', level=logging.INFO,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# Definir las claves de acceso API\n",
    "shared_key = 'wY0e3zria06ULMWuSmR2'\n",
    "secret_key = 'MJ4RWDjzZxtA36KBZh5LMlDKMrw9m0cHpfRzrgH0'\n",
    "\n",
    "# Leer el DataFrame de filer_ids desde un archivo CSV\n",
    "filers = pd.read_csv('filers.csv')\n",
    "\n",
    "# Renombrar la columna 'id' a 'filer_id'\n",
    "filers = filers.rename(columns={'id': 'filer_id'})\n",
    "\n",
    "# Lista para almacenar los holdings\n",
    "holdings_list = []\n",
    "\n",
    "# Lista para almacenar los tiempos de extracción\n",
    "extraction_times = []\n",
    "\n",
    "def fetch_holdings(filer_id):\n",
    "    logging.info(f\"Procesando filer_id: {filer_id}\")\n",
    "    \n",
    "    json_args = f'{{\"command\":\"holdings\",\"filer_ids\":[{filer_id}],\"quarter_ids\":[96,97],\"all_quarters\":0,\"include_13d\":1}}'\n",
    "    formatted_args = quote_plus(json_args)\n",
    "    timenow = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())\n",
    "\n",
    "    digest = hashlib.sha1\n",
    "    raw_args = json_args + '\\n' + timenow\n",
    "    hmac_hash = hmac.new(secret_key.encode(), raw_args.encode(), digest).digest()\n",
    "    sig = base64.b64encode(hmac_hash).rstrip()\n",
    "\n",
    "    url_base = 'https://whalewisdom.com/shell/command.json?'\n",
    "    url_args = 'args=' + formatted_args\n",
    "    url_end = f'&api_shared_key={shared_key}&api_sig={sig.decode()}&timestamp={timenow}'\n",
    "    api_url = url_base + url_args + url_end\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Realizando solicitud a la API para filer_id: {filer_id}\")\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()  # Lanza una excepción para códigos de estado HTTP 4xx/5xx\n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "\n",
    "        for result in results:\n",
    "            filer_name = result['filer_name']\n",
    "            for record in result['records']:\n",
    "                for holding in record['holdings']:\n",
    "                    holding['filer_name'] = filer_name\n",
    "                    holdings_list.append(holding)\n",
    "        logging.info(f\"Datos obtenidos para filer_id: {filer_id}\")\n",
    "        logging.info(f\"Consulta exitosa para filer_id: {filer_id}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error al conectar con la API de Whale Wisdom para filer_id {filer_id}: {e}\")\n",
    "        print(f\"Error al conectar con la API de Whale Wisdom para filer_id {filer_id}: {e}\")\n",
    "\n",
    "# Iterar sobre todos los registros en filers de forma secuencial\n",
    "start_time = datetime.now()\n",
    "\n",
    "#for index, row in filers.iterrows():\n",
    "fetch_holdings(71)\n",
    "\n",
    "end_time = datetime.now()\n",
    "extraction_time = (end_time - start_time).total_seconds()\n",
    "extraction_times.append({'total_records': len(filers), 'extraction_time': extraction_time})\n",
    "logging.info(f\"Tiempo total de extracción para {len(filers)} filer_ids: {extraction_time} segundos\")\n",
    "\n",
    "# Almacenar la información en un DataFrame\n",
    "holdings_df = pd.DataFrame(holdings_list)\n",
    "logging.info(\"Datos almacenados en el DataFrame\")\n",
    "print(holdings_df)\n",
    "\n",
    "# Crear un DataFrame con los tiempos de extracción\n",
    "extraction_times_df = pd.DataFrame(extraction_times)\n",
    "print(extraction_times_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import quote_plus\n",
    "import hmac\n",
    "import hashlib\n",
    "import base64\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurar el logger\n",
    "logging.basicConfig(filename='whalewisdom_api.log', level=logging.INFO,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# Definir las claves de acceso API\n",
    "shared_key = 'wY0e3zria06ULMWuSmR2'\n",
    "secret_key = 'MJ4RWDjzZxtA36KBZh5LMlDKMrw9m0cHpfRzrgH0'\n",
    "\n",
    "# Función para generar la URL de la API\n",
    "def generate_api_url(command, shared_key, secret_key):\n",
    "    json_args = f'{{\"command\":\"{command}\"}}'\n",
    "    formatted_args = quote_plus(json_args)\n",
    "    timenow = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    raw_args = f'{json_args}\\n{timenow}'\n",
    "    hmac_hash = hmac.new(secret_key.encode(), raw_args.encode(), hashlib.sha1).digest()\n",
    "    sig = base64.b64encode(hmac_hash).decode().rstrip()\n",
    "    return (f'https://whalewisdom.com/shell/command.json?args={formatted_args}'\n",
    "            f'&api_shared_key={shared_key}&api_sig={sig}&timestamp={timenow}')\n",
    "\n",
    "# Función para realizar la solicitud a la API\n",
    "def fetch_data_from_api(api_url):\n",
    "    try:\n",
    "        logging.info(f\"Realizando solicitud a la API: {api_url}\")\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error al conectar con la API: {e}\")\n",
    "        print(f\"Error al conectar con la API: {e}\")\n",
    "        return None\n",
    "\n",
    "# Procesar los datos de los quarters\n",
    "def process_quarters(data):\n",
    "    if not data or 'quarters' not in data:\n",
    "        logging.warning(\"No se encontraron datos de quarters en la respuesta.\")\n",
    "        return []\n",
    "\n",
    "    quarters = data['quarters']\n",
    "    logging.info(f\"Se obtuvieron {len(quarters)} quarters.\")\n",
    "    return quarters\n",
    "\n",
    "# Medir el tiempo de extracción\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Generar la URL y obtener los datos\n",
    "api_url = generate_api_url(\"quarters\", shared_key, secret_key)\n",
    "data = fetch_data_from_api(api_url)\n",
    "\n",
    "# Procesar los datos obtenidos\n",
    "quarter_list = process_quarters(data)\n",
    "\n",
    "# Calcular el tiempo de extracción\n",
    "end_time = datetime.now()\n",
    "extraction_time = (end_time - start_time).total_seconds()\n",
    "logging.info(f\"Tiempo total de extracción: {extraction_time} segundos\")\n",
    "\n",
    "# Crear un DataFrame con los quarters\n",
    "quarters_df = pd.DataFrame(quarter_list)\n",
    "print(quarters_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import quote_plus\n",
    "import hmac\n",
    "import hashlib\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Configurar el logger\n",
    "logging.basicConfig(filename='whalewisdom_api.log', level=logging.INFO,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# Definir las claves de acceso API\n",
    "shared_key = 'wY0e3zria06ULMWuSmR2'\n",
    "secret_key = 'MJ4RWDjzZxtA36KBZh5LMlDKMrw9m0cHpfRzrgH0'\n",
    "\n",
    "# Archivo para almacenar los quarters previamente procesados\n",
    "quarters_file = 'quarters.csv'\n",
    "\n",
    "# Función para generar la URL de la API\n",
    "def generate_api_url(command, shared_key, secret_key, filer_id=None):\n",
    "    if command == \"quarters\":\n",
    "        json_args = f'{{\"command\":\"{command}\"}}'\n",
    "    elif command == \"holdings\" and filer_id:\n",
    "        json_args = f'{{\"command\":\"holdings\",\"filer_ids\":[{filer_id}],\"include_13d\":1}}'\n",
    "    else:\n",
    "        raise ValueError(\"Comando o argumentos inválidos para generar la URL.\")\n",
    "    \n",
    "    formatted_args = quote_plus(json_args)\n",
    "    timenow = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    raw_args = f'{json_args}\\n{timenow}'\n",
    "    hmac_hash = hmac.new(secret_key.encode(), raw_args.encode(), hashlib.sha1).digest()\n",
    "    sig = base64.b64encode(hmac_hash).decode().rstrip()\n",
    "    return (f'https://whalewisdom.com/shell/command.json?args={formatted_args}'\n",
    "            f'&api_shared_key={shared_key}&api_sig={sig}&timestamp={timenow}')\n",
    "\n",
    "# Función para realizar la solicitud a la API\n",
    "def fetch_data_from_api(api_url):\n",
    "    try:\n",
    "        logging.info(f\"Realizando solicitud a la API: {api_url}\")\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error al conectar con la API: {e}\")\n",
    "        return None\n",
    "\n",
    "# Función para procesar los datos de quarters\n",
    "def process_quarters(data):\n",
    "    if not data or 'quarters' not in data:\n",
    "        logging.warning(\"No se encontraron datos de quarters en la respuesta.\")\n",
    "        return []\n",
    "    quarters = data['quarters']\n",
    "    logging.info(f\"Se obtuvieron {len(quarters)} quarters.\")\n",
    "    return quarters\n",
    "\n",
    "# Función para verificar si hay nuevos quarters\n",
    "def has_new_quarters(current_quarters):\n",
    "    if os.path.exists(quarters_file):\n",
    "        previous_quarters = pd.read_csv(quarters_file)['quarters'].tolist()\n",
    "        if current_quarters == previous_quarters:\n",
    "            logging.info(\"No hay nuevos quarters disponibles. Finalizando la ejecución.\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Función para guardar los quarters actuales\n",
    "def save_quarters(current_quarters):\n",
    "    pd.DataFrame({'quarters': current_quarters}).to_csv(quarters_file, index=False)\n",
    "    logging.info(\"Quarters actuales guardados en el archivo.\")\n",
    "\n",
    "# Función para procesar holdings de un filer_id\n",
    "def fetch_holdings(filer_id):\n",
    "    api_url = generate_api_url(\"holdings\", shared_key, secret_key, filer_id=filer_id)\n",
    "    data = fetch_data_from_api(api_url)\n",
    "    if not data:\n",
    "        return []\n",
    "    \n",
    "    holdings = []\n",
    "    results = data.get('results', [])\n",
    "    for result in results:\n",
    "        filer_name = result['filer_name']\n",
    "        for record in result['records']:\n",
    "            for holding in record['holdings']:\n",
    "                holding['filer_name'] = filer_name\n",
    "                holdings.append(holding)\n",
    "    logging.info(f\"Datos obtenidos para filer_id: {filer_id}\")\n",
    "    return holdings\n",
    "\n",
    "# Medir el tiempo de extracción\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Obtener y procesar los datos de quarters\n",
    "quarters_api_url = generate_api_url(\"quarters\", shared_key, secret_key)\n",
    "data = fetch_data_from_api(quarters_api_url)\n",
    "quarter_list = process_quarters(data)\n",
    "\n",
    "# Verificar si hay nuevos quarters\n",
    "if not quarter_list or has_new_quarters(quarter_list):\n",
    "    print(\"No hay nueva información de quarters. Finalizando la ejecución.\")\n",
    "else:\n",
    "    # Guardar los quarters actuales\n",
    "    save_quarters(quarter_list)\n",
    "\n",
    "    # Leer el DataFrame de filer_ids desde un archivo CSV\n",
    "    filers = pd.read_csv('filers.csv')\n",
    "    filers = filers.rename(columns={'id': 'filer_id'})\n",
    "    filers = filers.iloc[[9]]  # Solo para pruebas, eliminar esta línea en producción\n",
    "\n",
    "    # Lista para almacenar los holdings\n",
    "    holdings_list = []\n",
    "\n",
    "    # Iterar sobre todos los registros en filers\n",
    "    for index, row in filers.iterrows():\n",
    "        filer_id = row['filer_id']\n",
    "        holdings = fetch_holdings(filer_id)  # Ajusta los quarter_ids según sea necesario\n",
    "        holdings_list.extend(holdings)\n",
    "\n",
    "    # Almacenar la información en un DataFrame\n",
    "    holdings_df = pd.DataFrame(holdings_list)\n",
    "    logging.info(\"Datos almacenados en el DataFrame\")\n",
    "    print(holdings_df)\n",
    "\n",
    "# Calcular el tiempo de extracción\n",
    "end_time = datetime.now()\n",
    "extraction_time = (end_time - start_time).total_seconds()\n",
    "logging.info(f\"Tiempo total de extracción: {extraction_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import quote_plus\n",
    "import hmac\n",
    "import hashlib\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Configurar el logger\n",
    "logging.basicConfig(filename='whalewisdom_api.log', level=logging.INFO,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# Definir las claves de acceso API\n",
    "shared_key = 'wY0e3zria06ULMWuSmR2'\n",
    "secret_key = 'MJ4RWDjzZxtA36KBZh5LMlDKMrw9m0cHpfRzrgH0'\n",
    "\n",
    "# Archivo para almacenar los quarters previamente procesados\n",
    "quarters_file = 'quarters.csv'\n",
    "\n",
    "# Función para generar la URL de la API\n",
    "def generate_api_url(command, shared_key, secret_key, filer_id=None):\n",
    "    if command == \"quarters\":\n",
    "        json_args = f'{{\"command\":\"{command}\"}}'\n",
    "    elif command == \"holdings\" and filer_id:\n",
    "        json_args = f'{{\"command\":\"holdings\",\"filer_ids\":[{filer_id}],\"include_13d\":1}}'\n",
    "    else:\n",
    "        raise ValueError(\"Comando o argumentos inválidos para generar la URL.\")\n",
    "    \n",
    "    formatted_args = quote_plus(json_args)\n",
    "    timenow = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    raw_args = f'{json_args}\\n{timenow}'\n",
    "    hmac_hash = hmac.new(secret_key.encode(), raw_args.encode(), hashlib.sha1).digest()\n",
    "    sig = base64.b64encode(hmac_hash).decode().rstrip()\n",
    "    return (f'https://whalewisdom.com/shell/command.json?args={formatted_args}'\n",
    "            f'&api_shared_key={shared_key}&api_sig={sig}&timestamp={timenow}')\n",
    "\n",
    "# Función para realizar la solicitud a la API\n",
    "def fetch_data_from_api(api_url):\n",
    "    try:\n",
    "        logging.info(f\"Realizando solicitud a la API: {api_url}\")\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error al conectar con la API: {e}\")\n",
    "        return None\n",
    "\n",
    "# Función para procesar los datos de quarters\n",
    "def process_quarters(data):\n",
    "    if not data or 'quarters' not in data:\n",
    "        logging.warning(\"No se encontraron datos de quarters en la respuesta.\")\n",
    "        return []\n",
    "    quarters = data['quarters']\n",
    "    logging.info(f\"Se obtuvieron {len(quarters)} quarters.\")\n",
    "    return quarters\n",
    "\n",
    "# Función para verificar si hay nuevos quarters\n",
    "def has_new_quarters(current_quarters):\n",
    "    if os.path.exists(quarters_file):\n",
    "        # Leer los quarters previamente almacenados\n",
    "        try:\n",
    "            previous_quarters = pd.read_csv(quarters_file)['quarters'].apply(eval).tolist()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al leer el archivo quarters.csv: {e}\")\n",
    "            return True  # Si no se puede leer el archivo, asumir que hay nuevos quarters\n",
    "\n",
    "        # Comparar los quarters actuales con los previos\n",
    "        if set(tuple(sorted(q.items())) for q in current_quarters) == set(tuple(sorted(q.items())) for q in previous_quarters):\n",
    "            logging.info(\"No hay nuevos quarters disponibles. Finalizando la ejecución.\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Función para guardar los quarters actuales\n",
    "def save_quarters(current_quarters):\n",
    "    try:\n",
    "        pd.DataFrame({'quarters': [str(q) for q in current_quarters]}).to_csv(quarters_file, index=False)\n",
    "        logging.info(\"Quarters actuales guardados en el archivo.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al guardar los quarters en el archivo: {e}\")\n",
    "\n",
    "# Función para procesar holdings de un filer_id\n",
    "def fetch_holdings(filer_id):\n",
    "    api_url = generate_api_url(\"holdings\", shared_key, secret_key, filer_id=filer_id)\n",
    "    data = fetch_data_from_api(api_url)\n",
    "    if not data:\n",
    "        return []\n",
    "    \n",
    "    holdings = []\n",
    "    results = data.get('results', [])\n",
    "    for result in results:\n",
    "        filer_name = result['filer_name']\n",
    "        for record in result['records']:\n",
    "            for holding in record['holdings']:\n",
    "                holding['filer_name'] = filer_name\n",
    "                holdings.append(holding)\n",
    "    logging.info(f\"Datos obtenidos para filer_id: {filer_id}\")\n",
    "    return holdings\n",
    "\n",
    "# Medir el tiempo de extracción\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Obtener y procesar los datos de quarters\n",
    "quarters_api_url = generate_api_url(\"quarters\", shared_key, secret_key)\n",
    "data = fetch_data_from_api(quarters_api_url)\n",
    "quarter_list = process_quarters(data)\n",
    "\n",
    "# Verificar si hay nuevos quarters\n",
    "if not quarter_list or not has_new_quarters(quarter_list):\n",
    "    print(\"No hay nueva información de quarters. Finalizando la ejecución.\")\n",
    "else:\n",
    "    # Guardar los quarters actuales\n",
    "    save_quarters(quarter_list)\n",
    "\n",
    "    # Leer el DataFrame de filer_ids desde un archivo CSV\n",
    "    filers = pd.read_csv('filers.csv')\n",
    "    filers = filers.rename(columns={'id': 'filer_id'})\n",
    "\n",
    "    # Lista para almacenar los holdings\n",
    "    holdings_list = []\n",
    "\n",
    "    # Iterar sobre todos los registros en filers\n",
    "    for index, row in filers.iterrows():\n",
    "        filer_id = row['filer_id']\n",
    "        holdings = fetch_holdings(filer_id)\n",
    "        holdings_list.extend(holdings)\n",
    "\n",
    "    # Almacenar la información en un DataFrame\n",
    "    holdings_df = pd.DataFrame(holdings_list)\n",
    "    logging.info(\"Datos almacenados en el DataFrame\")\n",
    "    print(holdings_df)\n",
    "\n",
    "# Calcular el tiempo de extracción\n",
    "end_time = datetime.now()\n",
    "extraction_time = (end_time - start_time).total_seconds()\n",
    "logging.info(f\"Tiempo total de extracción: {extraction_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Cargar CSV en DataFrame\n",
    "df = pd.read_csv('quarters_DF.csv',sep=';')\n",
    "\n",
    "# Conectar a MySQL\n",
    "conn = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            database='Portafolio',\n",
    "            user='root',\n",
    "            password='P4ssw0rd!'\n",
    "        )\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Strip leading/trailing spaces from the 'filing_period' column\n",
    "df['filing_period'] = df['filing_period'].str.strip()\n",
    "\n",
    "# Convert 'filing_period' to the format 'YYYY-MM-DD'\n",
    "df['filing_period'] = pd.to_datetime(df['filing_period'], format='%m-%d-%Y').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Insert rows one by one\n",
    "for _, row in df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO quarters (quarter_id,filing_period,Status)\n",
    "        VALUES (%s,%s,%s)\n",
    "    \"\"\", (row['quarter_id'], row['filing_period'], row['Status']))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Cargar CSV en DataFrame\n",
    "df = pd.read_csv('holding_df.csv', sep=',')\n",
    "\n",
    "# Conectar a MySQL\n",
    "conn = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            database='Portafolio',\n",
    "            user='root',\n",
    "            password='P4ssw0rd!'\n",
    "        )\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Replace NaN values with None (interpreted as NULL in MySQL)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    print(\"Valores esperados:\", (\n",
    "        row['filer_id'], row['filer_name'], row['stock_id'], row['stock_name'], row['stock_ticker'],\n",
    "        row['security_type'], row['shares_change'], row['position_change_type'], \n",
    "        row['current_ranking'] if pd.notnull(row['current_ranking']) else None,\n",
    "        row['previous_ranking'] if pd.notnull(row['previous_ranking']) else None,\n",
    "        row['current_percent_of_portfolio'] if pd.notnull(row['current_percent_of_portfolio']) else None,\n",
    "        row['previous_percent_of_portfolio'] if pd.notnull(row['previous_percent_of_portfolio']) else None,\n",
    "        row['current_mv'] if pd.notnull(row['current_mv']) else None,\n",
    "        row['previous_mv'] if pd.notnull(row['previous_mv']) else None,\n",
    "        row['current_shares'] if pd.notnull(row['current_shares']) else None,\n",
    "        row['previous_shares'] if pd.notnull(row['previous_shares']) else None,\n",
    "        row['source_date'], row['source'], row['sector'], row['industry'], \n",
    "        row['percent_ownership'] if pd.notnull(row['percent_ownership']) else None,\n",
    "        row['filer_street_address'], row['filer_city'], row['filer_state'], row['filer_zip_code'], \n",
    "        row['avg_price'] if pd.notnull(row['avg_price']) else None,\n",
    "        row['percent_change'] if pd.notnull(row['percent_change']) else None,\n",
    "        row['quarter_id_owned'] if pd.notnull(row['quarter_id_owned']) else None,\n",
    "        row['quarter_end_price'] if pd.notnull(row['quarter_end_price']) else None\n",
    "    ))\n",
    "# # Insert rows one by one\n",
    "# for _, row in df.iterrows():\n",
    "#     cursor.execute(\"\"\"\n",
    "#         INSERT IGNORE INTO holding (filer_id, filer_name, stock_id, stock_name, stock_ticker, security_type, shares_change, position_change_type, current_ranking, previous_ranking, current_percent_of_portfolio, previous_percent_of_portfolio, current_mv, previous_mv, current_shares, previous_shares, source_date, source, sector, industry, percent_ownership, filer_street_address, filer_city, filer_state, filer_zip_code, avg_price, percent_change, quarter_id_owned, quarter_end_price)\n",
    "#         VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "#     \"\"\", (\n",
    "#         row['filer_id'], row['filer_name'], row['stock_id'], row['stock_name'], row['stock_ticker'],\n",
    "#         row['security_type'], row['shares_change'], row['position_change_type'], \n",
    "#         row['current_ranking'] if pd.notnull(row['current_ranking']) else None,\n",
    "#         row['previous_ranking'] if pd.notnull(row['previous_ranking']) else None,\n",
    "#         row['current_percent_of_portfolio'] if pd.notnull(row['current_percent_of_portfolio']) else None,\n",
    "#         row['previous_percent_of_portfolio'] if pd.notnull(row['previous_percent_of_portfolio']) else None,\n",
    "#         row['current_mv'] if pd.notnull(row['current_mv']) else None,\n",
    "#         row['previous_mv'] if pd.notnull(row['previous_mv']) else None,\n",
    "#         row['current_shares'] if pd.notnull(row['current_shares']) else None,\n",
    "#         row['previous_shares'] if pd.notnull(row['previous_shares']) else None,\n",
    "#         row['source_date'], row['source'], row['sector'], row['industry'], \n",
    "#         row['percent_ownership'] if pd.notnull(row['percent_ownership']) else None,\n",
    "#         row['filer_street_address'], row['filer_city'], row['filer_state'], row['filer_zip_code'], \n",
    "#         row['avg_price'] if pd.notnull(row['avg_price']) else None,\n",
    "#         row['percent_change'] if pd.notnull(row['percent_change']) else None,\n",
    "#         row['quarter_id_owned'] if pd.notnull(row['quarter_id_owned']) else None,\n",
    "#         row['quarter_end_price'] if pd.notnull(row['quarter_end_price']) else None\n",
    "#     ))\n",
    "#     conn.commit()  # Commit after each insertion\n",
    "#     print(f\"Fila insertada: {row['filer_id']} - {row['stock_name']}\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Cargar CSV en DataFrame\n",
    "df = pd.read_csv('holding_df.csv', sep=',')\n",
    "\n",
    "# Conectar a MySQL\n",
    "conn = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            database='Portafolio',\n",
    "            user='root',\n",
    "            password='P4ssw0rd!'\n",
    "        )\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Replace NaN values with None (interpreted as NULL in MySQL)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "\n",
    "# Imprimir las columnas del DataFrame\n",
    "print(\"Columnas del DataFrame:\", df.columns)\n",
    "\n",
    "# Verificar valores esperados y columnas faltantes\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        valores = (\n",
    "            row['filer_id'], row['filer_name'], row['stock_id'], row['stock_name'], row['stock_ticker'],\n",
    "            row['security_type'], row['shares_change'], row['position_change_type'], \n",
    "            row['current_ranking'] if pd.notnull(row['current_ranking']) else None,\n",
    "            row['previous_ranking'] if pd.notnull(row['previous_ranking']) else None,\n",
    "            row['current_percent_of_portfolio'] if pd.notnull(row['current_percent_of_portfolio']) else None,\n",
    "            row['previous_percent_of_portfolio'] if pd.notnull(row['previous_percent_of_portfolio']) else None,\n",
    "            row['current_mv'] if pd.notnull(row['current_mv']) else None,\n",
    "            row['previous_mv'] if pd.notnull(row['previous_mv']) else None,\n",
    "            row['current_shares'] if pd.notnull(row['current_shares']) else None,\n",
    "            row['previous_shares'] if pd.notnull(row['previous_shares']) else None,\n",
    "            row['source_date'], row['source'], row['sector'], row['industry'], \n",
    "            row['percent_ownership'] if pd.notnull(row['percent_ownership']) else None,\n",
    "            row['filer_street_address'], row['filer_city'], row['filer_state'], row['filer_zip_code'], \n",
    "            row['avg_price'] if pd.notnull(row['avg_price']) else None,\n",
    "            row['percent_change'] if pd.notnull(row['percent_change']) else None,\n",
    "            row['quarter_id_owned'] if pd.notnull(row['quarter_id_owned']) else None,\n",
    "            row['quarter_end_price'] if pd.notnull(row['quarter_end_price']) else None\n",
    "        )\n",
    "        print(\"Valores esperados:\", valores)\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Falta la columna {e} en el DataFrame.\")\n",
    "        break\n",
    "\n",
    "# # Insert rows one by one\n",
    "# for _, row in df.iterrows():\n",
    "#     cursor.execute(\"\"\"\n",
    "#         INSERT IGNORE INTO holding (filer_id, filer_name, stock_id, stock_name, stock_ticker, security_type, shares_change, position_change_type, current_ranking, previous_ranking, current_percent_of_portfolio, previous_percent_of_portfolio, current_mv, previous_mv, current_shares, previous_shares, source_date, source, sector, industry, percent_ownership, filer_street_address, filer_city, filer_state, filer_zip_code, avg_price, percent_change, quarter_id_owned, quarter_end_price)\n",
    "#         VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "#     \"\"\", valores)\n",
    "#     conn.commit()  # Commit after each insertion\n",
    "#     print(f\"Fila insertada: {row['filer_id']} - {row['stock_name']}\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Cargar CSV en DataFrame\n",
    "# Use on_bad_lines='skip' to skip problematic rows\n",
    "df = pd.read_csv('filers.csv')\n",
    "# Alternatively, if you want to debug the issue, you can read the file without parsing and inspect it:\n",
    "# with open('filers.csv', 'r') as file:\n",
    "#     lines = file.readlines()\n",
    "#     for i, line in enumerate(lines):\n",
    "#         print(f\"Line {i + 1}: {line.strip()}\")\n",
    "\n",
    "# Conectar a MySQL\n",
    "conn = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            database='Portafolio',\n",
    "            user='root',\n",
    "            password='P4ssw0rd!'\n",
    "        )\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Replace NaN values with None (interpreted as NULL in MySQL)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "\n",
    "# Insert rows one by one and commit after each insertion\n",
    "for _, row in df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT IGNORE INTO filers (filer_id,name,cik,street_address,state,zip_code,business_phone,permalink)\n",
    "        VALUES (%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "    \"\"\", (row['id'], row['name'], row['cik'], row['street_address'], row['state'], row['zip_code'], row['business_phone'], row['permalink']))\n",
    "    conn.commit()  # Commit after each insertion\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "try:\n",
    "    conn = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            database='Portafolio',\n",
    "            user='root',\n",
    "            password='P4ssw0rd!'\n",
    "        )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Ejecutar alguna consulta\n",
    "    cursor.execute(\"\"\"\n",
    "ALTER TABLE `Portafolio`.`holding` \n",
    "CHANGE COLUMN `previous_share` `previous_shares` INT NULL DEFAULT NULL ;\n",
    "                   \"\"\")\n",
    "\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(\"Error:\", err)\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()  #  Esta línea cierra la conexión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =df[df['source_date']=='2025-04-30']\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
